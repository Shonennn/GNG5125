{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "import csv\n",
    "import gensim\n",
    "import warnings \n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import feature_extraction, manifold\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import cohen_kappa_score, silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "warnings.filterwarnings('ignore') # to ignore deprecated functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhouz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zhouz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhouz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Selection\n",
    "Take five different samples of Gutenberg digital books, which are all of five different genres and of five different authors, that are semantically different.\n",
    "\n",
    "So we choice:\n",
    "- Peter Pan by J. M. Barrie, and genre: Children's Literature. Semantic category: Fantasy, Adventure.\n",
    "- The Origin of Species by Charles Darwin, and genre: Science. Semantic category: Evolution, Biology, Genetics.\n",
    "- War and Peace by Leo Tolstoy, and genre: Historical Fiction. Semantic category: War, Society, History.\n",
    "- Pride and Prejudice by Jane Austen, and genre: Romance Fiction. Semantic category: Romance, Social class, Society.\n",
    "- The Adventures of Sherlock Holmes by Arthur Conan Doyle, and genre: Mystery. Semantic category: Detective, Crime, Logic, Deduction, Suspense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter_Pan.txt by J. M. Barrie has been saved.\n",
      "On_the_Origin_of_Species.txt by Charles Darwin has been saved.\n",
      "War_and_Peace.txt by Leo Tolstoy has been saved.\n",
      "Pride_and_Prejudice.txt by Jane Austen has been saved.\n",
      "The_Adventures_of_Sherlock_Holmes.txt by Arthur Conan Doyle has been saved.\n"
     ]
    }
   ],
   "source": [
    "# Define the URL and filenames for each book\n",
    "books = [ \n",
    "         {\"url\": \"https://www.gutenberg.org/files/16/16-0.txt\", \"filename\": \"Peter_Pan.txt\", \"author\": \"J. M. Barrie\", \"genre\": \"Children's Literature\"},\n",
    "         {\"url\": \"https://www.gutenberg.org/cache/epub/1228/pg1228.txt\", \"filename\": \"On_the_Origin_of_Species.txt\", \"author\": \"Charles Darwin\", \"genre\": \"Science\"},\n",
    "         {\"url\": \"https://www.gutenberg.org/cache/epub/2600/pg2600.txt\", \"filename\": \"War_and_Peace.txt\", \"author\": \"Leo Tolstoy\", \"genre\": \"Historical Fiction\"},\n",
    "         {\"url\": \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\", \"filename\": \"Pride_and_Prejudice.txt\", \"author\": \"Jane Austen\", \"genre\": \"Romance Fiction\"},\n",
    "         {\"url\": \"https://www.gutenberg.org/files/1661/1661-0.txt\", \"filename\": \"The_Adventures_of_Sherlock_Holmes.txt\", \"author\": \"Arthur Conan Doyle\", \"genre\": \"Mystery\"},\n",
    "]\n",
    "\n",
    "\n",
    "# Download and save each book\n",
    "for book in books:\n",
    "    url = book[\"url\"]\n",
    "    filename = book[\"filename\"]\n",
    "    author = book[\"author\"]\n",
    "    genre = book[\"genre\"]\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "    print(f\"{filename} by {author} has been saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepare and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_digital_book(book_name, num_partitions, size_partition):\n",
    "    \n",
    "    # Download the digital book from the local directory\n",
    "    book_file = open(book_name, \"r\", encoding='utf-8')\n",
    "    book = book_file.read()\n",
    "    book_file.close()\n",
    "\n",
    "    # use 'word_tokenize' function to tokenize the book into words.\n",
    "    partitions = nltk.word_tokenize(book)\n",
    "    partitions = [partitions[i : i+size_partition] for i in range(0, len(partitions), size_partition)]\n",
    "    \n",
    "    # Check num_partitions is valid\n",
    "    if num_partitions > len(partitions) or num_partitions < 0:\n",
    "        num_partitions = len(partitions)\n",
    "    partitions = partitions[:num_partitions]\n",
    "    \n",
    "    # Create labels\n",
    "    labels = [book_name[:1]]\n",
    "    # Repeat the labels for the number of times that can fit in the partitions\n",
    "    label_list = labels*(num_partitions//len(labels))\n",
    "    # Get the remainder labels that are needed.\n",
    "    label_list += labels[:num_partitions%len(labels)]\n",
    "    \n",
    "    # Use regular expression to manipulate the text\n",
    "    # and the regular expression r'[^\\w\\s]' is used to remove non-alphanumeric characters from the text.\n",
    "    partitions = [[re.sub(r'[^\\w\\s]', '', word) for word in partition] for partition in partitions]\n",
    "    \n",
    "    # Remove empty strings from the list\n",
    "    partitions = [[word for word in partition if word] for partition in partitions]\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    partitions = [[word for word in partition if word.lower() not in stop_words] for partition in partitions]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    partitions = [[stemmer.stem(word) for word in partition] for partition in partitions]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    partitions = [[lemmatizer.lemmatize(word) for word in partition] for partition in partitions]\n",
    "\n",
    "    # Create pandas dataframe to store the text data\n",
    "    data = {'partition': partitions, 'label': label_list}\n",
    "    df = pd.DataFrame(data) \n",
    "    \n",
    "    return partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for multiple books:\n",
    "book_name1 = \"Peter_Pan.txt\"\n",
    "book_name2 = \"On_the_Origin_of_Species.txt\"\n",
    "book_name3 = \"War_and_Peace.txt\"\n",
    "book_name4 = \"Pride_and_Prejudice.txt\"\n",
    "book_name5 = \"The_Adventures_of_Sherlock_Holmes.txt\"\n",
    "book_list = [book_name1, book_name2, book_name3, book_name4, book_name5]\n",
    "# If book_list > label_list, add \"f\",\"g\",etc.\n",
    "label_list = [\"a\",\"b\",\"c\",\"d\",\"e\"]\n",
    "df_list = []\n",
    "\n",
    "# samples 200 documents of each book, with 150 words records for each document. \n",
    "for i, book_name in enumerate(book_list):\n",
    "    partitions = sample_digital_book(book_name, 200, 150)\n",
    "    temp = pd.DataFrame({'partition': partitions, 'label': label_list[i]})\n",
    "    df_list.append(temp)\n",
    "\n",
    "df = pd.concat(df_list)\n",
    "df.to_csv(\"books_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
